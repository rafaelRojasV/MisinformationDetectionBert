system:
  run_name: "fusion_microsoft-deberta-v3-base_2cls_tfidf_meta_lora"
  logging:
    log_level: "INFO"

model:
  name: "microsoft/deberta-v3-base"
  num_labels: 2

  # LoRA hyperparams
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_bias: "none"

  # For DeBERTa-v3, typical submodules include query_proj, key_proj, value_proj in attention.
  # The feed-forward can often be matched via "intermediate.dense" and "output.dense".
  lora_target_modules:
    - "query_proj"
    - "key_proj"
    - "value_proj"
    - "intermediate.dense"
    - "output.dense"

tfidf:
  use: true
  max_features: 2000
  ngram_range: [1, 3]
  sublinear_tf: true
  min_df: 3
  scale: true

metadata:
  use: true
  scale: true

training:
  sample_size: null
  use_8bit: false
  gradient_checkpointing: false
  use_fp16: true
  device: "cuda"

  learning_rate: 3e-5
  batch_size: 16
  gradient_accumulation_steps: 1
  num_train_epochs: 4
  max_length: 256

  label_smoothing_factor: 0.0
  early_stopping_patience: 2
  output_dir: "peft_fusion_output"
  return_logits: false
  inference_batch_size: 32

evaluation:
  thresholds_to_try: [0.3, 0.5, 0.7]
  authenticity_mapping: null
  n_samples_for_pca: 1000
