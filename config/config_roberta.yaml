system:
  run_name: "fusion_roberta-base_2cls_tfidf_meta_lora"
  logging:
    log_level: "INFO"

model:
  name: "roberta-base"
  num_labels: 2

  # LoRA hyperparams
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_bias: "none"

  # In RoBERTa, attention layers have q_proj, k_proj, v_proj, out_proj
  # The feed-forward is in intermediate.dense and output.dense
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    - "intermediate.dense"
    - "output.dense"

tfidf:
  use: true
  max_features: 2000
  ngram_range: [1, 3]
  sublinear_tf: true
  min_df: 3
  scale: true

metadata:
  use: false
  scale: none

training:
  sample_size: null
  use_8bit: false
  gradient_checkpointing: false
  use_fp16: true
  device: "cuda"

  learning_rate: 3e-5
  batch_size: 16
  gradient_accumulation_steps: 1
  num_train_epochs: 4
  max_length: 256

  label_smoothing_factor: 0.0
  early_stopping_patience: 2
  output_dir: "peft_fusion_output"
  return_logits: false
  inference_batch_size: 32

evaluation:
  thresholds_to_try: [0.3, 0.5, 0.7]
  authenticity_mapping: null
  n_samples_for_pca: 1000
